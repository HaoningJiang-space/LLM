{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies: {'in', 'dog', 'bird', 'mat', 'on', 'hat', 'tree.', 'the', 'cat'}\n",
      "Word to Index Mapping: {'in': 0, 'dog': 1, 'bird': 2, 'mat': 3, 'on': 4, 'hat': 5, 'tree.': 6, 'the': 7, 'cat': 8}\n",
      "One Hot Encoded Matrix: [[0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
      "cat: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "in: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "the: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "hat: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "dog: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "on: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "the: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "mat: [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "bird: [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "in: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "the: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "tree.: [0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(text):\n",
    "    words = text.split()\n",
    "    vocabularies = set(words)\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabularies)}\n",
    "    one_hot_encoded = []\n",
    "    for word in words:\n",
    "        one_hot_vector = [0] * len(vocabularies)\n",
    "        one_hot_vector[word_to_index[word]] = 1\n",
    "        one_hot_encoded.append(one_hot_vector)\n",
    "    return one_hot_encoded, vocabularies, word_to_index\n",
    "\n",
    "# Test\n",
    "example_text = \"cat in the hat dog on the mat bird in the tree.\"\n",
    "one_hot_encoded, vocabularies, word_to_index = one_hot_encode(example_text)\n",
    "print(\"Vocabularies:\", vocabularies)\n",
    "print(\"Word to Index Mapping:\", word_to_index)\n",
    "print(\"One Hot Encoded Matrix:\", one_hot_encoded)\n",
    "for word, encoding in zip(example_text.split(), one_hot_encoded):\n",
    "    print(f\"{word}: {encoding}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocabulary(Feature name): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\"\"\" \n",
    "CountVectorizer is a class in the scikit-learn library that is used to convert a collection of text documents to a matrix of token counts. It is important to note that CountVectorizer converts text documents to a matrix of token counts, not one-hot encoded vectors.\n",
    "Different from one-hot encoding, CountVectorizer does not create a binary vector. Instead, it counts the frequency of each word in the text documents.\n",
    "\n",
    "\"\"\"\n",
    "documents = [\"This is the first document.\",\"This document is the second document.\",\"And this is the third one.\",\"Is this the first document?\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Bag-of-words matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary(Feature name):\" , feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding-TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "0: {'dog': 0.30151134457776363, 'lazy': 0.30151134457776363, 'over': 0.30151134457776363, 'jumps': 0.30151134457776363, 'fox': 0.30151134457776363, 'brown': 0.30151134457776363, 'quick': 0.30151134457776363, 'the': 0.6030226891555273}\n",
      "\n",
      "\n",
      "1: {'step': 0.3535533905932738, 'single': 0.3535533905932738, 'with': 0.3535533905932738, 'begins': 0.3535533905932738, 'miles': 0.3535533905932738, 'thousand': 0.3535533905932738, 'of': 0.3535533905932738, 'journey': 0.3535533905932738}\n",
      "\n",
      "\n",
      "Document 2:\n",
      "0: {'dog': 0.30151134457776363, 'lazy': 0.30151134457776363, 'over': 0.30151134457776363, 'jumps': 0.30151134457776363, 'fox': 0.30151134457776363, 'brown': 0.30151134457776363, 'quick': 0.30151134457776363, 'the': 0.6030226891555273}\n",
      "\n",
      "\n",
      "1: {'step': 0.3535533905932738, 'single': 0.3535533905932738, 'with': 0.3535533905932738, 'begins': 0.3535533905932738, 'miles': 0.3535533905932738, 'thousand': 0.3535533905932738, 'of': 0.3535533905932738, 'journey': 0.3535533905932738}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\"\"\"\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. TF-IDF is a product of two statistics: term frequency and inverse document frequency.\n",
    "\"\"\"\n",
    "# Sample text\n",
    "documents = [\"The quick brown fox jumps over the lazy dog.\",\" A journey of a thousand miles begins with a single step.\",]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = {}\n",
    "\n",
    "for doc_index, doc in enumerate(documents):\n",
    "    feature_index = tfidf_matrix[doc_index,:].nonzero()[1]\n",
    "    tfidf_doc_values = zip(feature_index, [tfidf_matrix[doc_index, x] for x in feature_index])\n",
    "    tfidf_values[doc_index] = {feature_names[i]: value for i, value in tfidf_doc_values}\n",
    "    \n",
    "for doc_index,doc in tfidf_values.items():\n",
    "    print(f\"Document {doc_index+1}:\")\n",
    "    for word, tfidf_value in tfidf_values.items():\n",
    "        print(f\"{word}: {tfidf_value}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding-CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0\n",
      "Epoch 2, Loss: 0\n",
      "Epoch 3, Loss: 0\n",
      "Epoch 4, Loss: 0\n",
      "Epoch 5, Loss: 0\n",
      "Epoch 6, Loss: 0\n",
      "Epoch 7, Loss: 0\n",
      "Epoch 8, Loss: 0\n",
      "Epoch 9, Loss: 0\n",
      "Epoch 10, Loss: 0\n",
      "Epoch 11, Loss: 0\n",
      "Epoch 12, Loss: 0\n",
      "Epoch 13, Loss: 0\n",
      "Epoch 14, Loss: 0\n",
      "Epoch 15, Loss: 0\n",
      "Epoch 16, Loss: 0\n",
      "Epoch 17, Loss: 0\n",
      "Epoch 18, Loss: 0\n",
      "Epoch 19, Loss: 0\n",
      "Epoch 20, Loss: 0\n",
      "Epoch 21, Loss: 0\n",
      "Epoch 22, Loss: 0\n",
      "Epoch 23, Loss: 0\n",
      "Epoch 24, Loss: 0\n",
      "Epoch 25, Loss: 0\n",
      "Epoch 26, Loss: 0\n",
      "Epoch 27, Loss: 0\n",
      "Epoch 28, Loss: 0\n",
      "Epoch 29, Loss: 0\n",
      "Epoch 30, Loss: 0\n",
      "Epoch 31, Loss: 0\n",
      "Epoch 32, Loss: 0\n",
      "Epoch 33, Loss: 0\n",
      "Epoch 34, Loss: 0\n",
      "Epoch 35, Loss: 0\n",
      "Epoch 36, Loss: 0\n",
      "Epoch 37, Loss: 0\n",
      "Epoch 38, Loss: 0\n",
      "Epoch 39, Loss: 0\n",
      "Epoch 40, Loss: 0\n",
      "Epoch 41, Loss: 0\n",
      "Epoch 42, Loss: 0\n",
      "Epoch 43, Loss: 0\n",
      "Epoch 44, Loss: 0\n",
      "Epoch 45, Loss: 0\n",
      "Epoch 46, Loss: 0\n",
      "Epoch 47, Loss: 0\n",
      "Epoch 48, Loss: 0\n",
      "Epoch 49, Loss: 0\n",
      "Epoch 50, Loss: 0\n",
      "Epoch 51, Loss: 0\n",
      "Epoch 52, Loss: 0\n",
      "Epoch 53, Loss: 0\n",
      "Epoch 54, Loss: 0\n",
      "Epoch 55, Loss: 0\n",
      "Epoch 56, Loss: 0\n",
      "Epoch 57, Loss: 0\n",
      "Epoch 58, Loss: 0\n",
      "Epoch 59, Loss: 0\n",
      "Epoch 60, Loss: 0\n",
      "Epoch 61, Loss: 0\n",
      "Epoch 62, Loss: 0\n",
      "Epoch 63, Loss: 0\n",
      "Epoch 64, Loss: 0\n",
      "Epoch 65, Loss: 0\n",
      "Epoch 66, Loss: 0\n",
      "Epoch 67, Loss: 0\n",
      "Epoch 68, Loss: 0\n",
      "Epoch 69, Loss: 0\n",
      "Epoch 70, Loss: 0\n",
      "Epoch 71, Loss: 0\n",
      "Epoch 72, Loss: 0\n",
      "Epoch 73, Loss: 0\n",
      "Epoch 74, Loss: 0\n",
      "Epoch 75, Loss: 0\n",
      "Epoch 76, Loss: 0\n",
      "Epoch 77, Loss: 0\n",
      "Epoch 78, Loss: 0\n",
      "Epoch 79, Loss: 0\n",
      "Epoch 80, Loss: 0\n",
      "Epoch 81, Loss: 0\n",
      "Epoch 82, Loss: 0\n",
      "Epoch 83, Loss: 0\n",
      "Epoch 84, Loss: 0\n",
      "Epoch 85, Loss: 0\n",
      "Epoch 86, Loss: 0\n",
      "Epoch 87, Loss: 0\n",
      "Epoch 88, Loss: 0\n",
      "Epoch 89, Loss: 0\n",
      "Epoch 90, Loss: 0\n",
      "Epoch 91, Loss: 0\n",
      "Epoch 92, Loss: 0\n",
      "Epoch 93, Loss: 0\n",
      "Epoch 94, Loss: 0\n",
      "Epoch 95, Loss: 0\n",
      "Epoch 96, Loss: 0\n",
      "Epoch 97, Loss: 0\n",
      "Epoch 98, Loss: 0\n",
      "Epoch 99, Loss: 0\n",
      "Epoch 100, Loss: 0\n"
     ]
    }
   ],
   "source": [
    "#Continuous Bag of Words (CBOW) \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the CBOW model\n",
    "\"\"\" \n",
    "Expalnation:\n",
    "embedding: The embedding layer is used to convert the input word indices into word embeddings. The embedding layer is initialized with random weights and is updated during the training process.\n",
    "linear: The linear layer is used to convert the sum of the word embeddings into a vector of size vocab_size. The linear layer is used to predict the target word.\n",
    "\n",
    "context_embeds: The context_embeds variable is used to sum the word embeddings of the context words. The context words are the input words that are used to predict the target word.\n",
    "output: The output variable is used to predict the target word using the linear layer. The output variable is the predicted word vector.\n",
    "\n",
    "\"\"\"\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context):\n",
    "        context_embeds = self.embeddings(context).sum(dim=1)\n",
    "        output = self.linear(context_embeds)\n",
    "        return output\n",
    "    \n",
    "# Sample data\n",
    "context_size = 2\n",
    "raw_text = \"word embedding are awesome\"\n",
    "tokens = raw_text.split()\n",
    "vocab = set(tokens)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)} #字典映射\n",
    "data = []\n",
    "for i in range(2,len(tokens)-2):\n",
    "    context = [word_to_index[word] for word in tokens[i-2:i]] + [word_to_index[word] for word in tokens[i+1:i+3]] #上下文, 2个前面的词和2个后面的词\n",
    "    target = word_to_index[tokens[i]] #目标词\n",
    "    data.append((context, target)) #数据集\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Initialize the CBOW model\n",
    "cbow_model = CBOWModel(vocab_size, embed_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the CBOW model\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context ,target in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = cbow_model(context)\n",
    "        loss = criterion(output.unsqeeze(0), target.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of the word 'fox': [ 7.6557780e-03  9.1921482e-03  1.1411518e-03 -8.3091129e-03\n",
      "  8.4182704e-03 -3.7443098e-03  5.7431986e-03  4.4432674e-03\n",
      "  9.6431728e-03 -9.3206121e-03  9.2158644e-03 -9.3190912e-03\n",
      " -6.9693117e-03 -9.1118161e-03 -5.5355071e-03  7.3632225e-03\n",
      "  9.1817779e-03 -3.3075348e-03  3.6922069e-03 -3.6953795e-03\n",
      "  7.8955982e-03  5.8766557e-03  6.1806844e-05 -3.6396664e-03\n",
      " -7.2316998e-03  4.7761607e-03  1.4220346e-03 -2.5958526e-03\n",
      "  7.8297537e-03 -4.0438161e-03 -9.1427332e-03 -2.3037302e-03\n",
      "  1.5958906e-04 -6.6816122e-03 -5.4892530e-03 -8.4930165e-03\n",
      "  9.2733549e-03  7.4490649e-03 -3.0979502e-04  7.3696864e-03\n",
      "  7.9537332e-03 -8.1320369e-04  6.5710987e-03  3.8147303e-03\n",
      "  5.0863335e-03  7.2565181e-03 -4.7544595e-03 -2.1836259e-03\n",
      "  8.9267868e-04  4.2488538e-03  3.3067961e-03  5.0703236e-03\n",
      "  4.6047554e-03 -8.4244283e-03 -3.2130813e-03 -7.2407331e-03\n",
      "  9.7420076e-03  4.9901991e-03  1.6234057e-04  4.1508423e-03\n",
      " -7.6533323e-03 -6.3014315e-03  3.1065089e-03  6.5242378e-03\n",
      "  3.9607380e-03  6.0531255e-03 -1.9433895e-03 -3.2796478e-03\n",
      "  1.8532290e-04 -3.1723732e-03 -5.5384813e-03 -7.7598062e-03\n",
      "  6.5626507e-03 -1.0743901e-03 -1.8563283e-03 -7.7587711e-03\n",
      "  9.3384096e-03  8.9243008e-04  1.7716525e-03  2.4401364e-03\n",
      " -7.4249967e-03  1.6299238e-03  3.0039453e-03 -8.5682208e-03\n",
      "  4.9505038e-03  2.4329762e-03  7.5145569e-03  5.0199446e-03\n",
      " -3.0151480e-03 -7.1538719e-03  7.1422239e-03  1.9340741e-03\n",
      "  5.2037169e-03  6.3590771e-03  1.9543641e-03 -6.1170463e-03\n",
      "  1.1349576e-05  8.2611330e-03 -6.1120638e-03  9.4829667e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\28154\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ! pip install gensim\n",
    "# ! pip install nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # Download the Punkt tokenizer\n",
    "\n",
    "# Sample text\n",
    "sample = \"The quick brown fox jumps over the lazy dog. A journey of a thousand miles begins with a single step.\"\n",
    "tokenized_corpus = word_tokenize(sample.lower()) #lowercasing for consistency\n",
    "\n",
    "skipgram_model = Word2Vec(sentences=[tokenized_corpus], \n",
    "                          vector_size=100, # Dimensionality of the word vectors\n",
    "                          window=5, # The maximum distance between the current and predicted word within a sentence\n",
    "                          sg=1, # Training algorithm: skip-gram\n",
    "                          min_count=1, # Ignores all words with total frequency lower than this\n",
    "                          workers=2)\n",
    "\n",
    "# Training \n",
    "skipgram_model.train([tokenized_corpus], total_examples=1, epochs=10)\n",
    "skipgram_model.save(\"skipgram.model.model\")\n",
    "loaded_model = Word2Vec.load(\"skipgram.model.model\")\n",
    "vector_representation = loaded_model.wv['fox']  # Get the vector representation of the word \"fox\"\n",
    "print(\"Vector representation of the word 'fox':\", vector_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'learn' and 'learning' using Glove: 0.802\n",
      "Similarity between 'india' and 'indian' using Glove: 0.865\n",
      "Similarity between 'fame' and 'famous' using Glove: 0.589\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import load\n",
    "\n",
    "glove_model = load(\"glove-wiki-gigaword-50\") # Load the GloVe model\n",
    "word_pairs = [('learn', 'learning'),('india','indian'),('fame','famous')]\n",
    "\n",
    "for pair in word_pairs:\n",
    "    similarity = glove_model.similarity(pair[0], pair[1])\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Glove: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext:常见"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
      "Similarity between 'learn' and 'learning' using FastText: 0.642\n",
      "Similarity between 'india' and 'indian' using FastText: 0.708\n",
      "Similarity between 'fame' and 'famous' using FastText: 0.519\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\") # Load the FastText model\n",
    "word_pairs = [('learn', 'learning'),('india','indian'),('fame','famous')]\n",
    "\n",
    "for pair in word_pairs:\n",
    "    similarity = fasttext_model.similarity(pair[0], pair[1])\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using FastText: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (6.7.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp37-cp37m-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.2/153.2 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.3-cp37-none-win_amd64.whl (287 kB)\n",
      "     -------------------------------------- 287.5/287.5 kB 8.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "     -------------------------------------- 268.8/268.8 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "     -------------------------------------- 143.0/143.0 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Installing collected packages: tokenizers, safetensors, pyyaml, fsspec, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.12.2 fsspec-2023.1.0 huggingface-hub-0.16.4 pyyaml-6.0.1 safetensors-0.4.3 tokenizers-0.13.3 transformers-4.30.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e866cf3567a447880ee44c770516d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\28154\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07624c8d20b647c9911f9c5660e950ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdeec125cfdc4c38b633e96615ab4e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863e19ed2e3e4d4984268ee102e5d193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "register_buffer() got an unexpected keyword argument 'persistent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23496\\1989632867.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mword_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'learn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'learning'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'india'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'indian'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fame'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'famous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m         \u001b[1;31m# Check first if we are `from_pt`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertEmbeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    891\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\28154\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"position_ids\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         self.register_buffer(\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[1;34m\"token_type_ids\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         )\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: register_buffer() got an unexpected keyword argument 'persistent'"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "word_pairs = [('learn', 'learning'),('india','indian'),('fame','famous')]\n",
    "for pair in word_pairs:\n",
    "    tokens = tokenizer(pair,retrun_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "       \n",
    "        # Extract embeddings for the [CLS] token    \n",
    "        cls_embedding = output.last_hidden_state[:,0,:]\n",
    "        \n",
    "        similarity = torch.cosine_similarity(cls_embedding[0], cls_embedding[1], dim=0)\n",
    "        print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using BERT: {similarity:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
